\documentclass[12pt,a4paper]{article}

\input{LaTex/packages.tex}

\usepackage{fancyhdr}
\setlength{\headheight}{15pt}  % Increase header height
\addtolength{\topmargin}{-3pt}  % Adjust top margin to compensate

\begin{document}

% Title page
\begin{titlepage}
\centering
\includegraphics[width=0.75\textwidth]{LaTex/imgs/bse_logo.pdf}
\par\vspace{0.75cm}
	{\huge\bfseries Assignment 4 \par}
    {\large\bfseries Foundations of Econometrics \\
                        Group 7\par}
	\vspace{0.25cm}
    \noindent\rule{\textwidth}{1pt}
    {\Large 
        \par}
    \noindent\rule{\textwidth}{1pt}
	\vfill
	{\large \today\par}
\end{titlepage}
\newpage

\section*{Question 1}
Class slide 3(33) (Unit 3) illustrated, via simulation, the effects of collinearity. The script used to generate the sample is included in file \texttt{data33.R}.

\subsection*{Part (a)}
\begin{enumerate}[label=(\roman*)]
  \item Estimate the regression model included in the slide, presenting OLS estimates and the 95\% confidence intervals for each parameter; Include the output in your answer.
  
  \textbf{Answer:} 
 
\begin{table}[ht]
    \centering
    \caption{Regression Results with 95\% CI} 
    \label{tab:regression}
  \begin{tabular}{rrrrr}
    \hline
    & Estimate & Std..Error & Lower & Upper \\ 
    \hline
    (Intercept) & 7.9365 & 1.6426 & 4.5863 & 11.2867 \\ 
    x2 & 0.5953 & 0.0747 & 0.4429 & 0.7476 \\ 
    x3 & 0.2996 & 0.5264 & -0.7740 & 1.3733 \\ 
    x4 & 0.7818 & 0.5277 & -0.2945 & 1.8582 \\ 
    \hline
  \end{tabular}
\end{table}

  
  \item Using \texttt{confidenceEllipse()} function, or equivalent, draw the 95\% confidence region for parameters $\beta_3$, $\beta_4$. Include also in the drawing the confidence intervals for each parameter.
  
  \textbf{Answer:} \textcolor{red}{The area inside the ellipse should be with color?? And in the legend some numbers don't apear}


    \begin{figure}[H]  % Capital H forces it exactly here
      \centering
      \includegraphics[width=0.4\textwidth]{Files/q1ii_plot.png}
      \caption{95\% Confidence Region for $\beta_3$ and $\beta_4$}
      \label{fig:ellipse}
    \end{figure}

  \item Describe what the confidence region you just drew provides.
  
  \textbf{Answer:} 

  The confidence region we drew illustrates the joint confiendence region, while the red lines show the individual confidence intervals.
  Its represent all the pair of values $\{x_i, y_i\}$ such that the joint test $H_0 : \beta_3 = x_i , \: \beta_4 = y_i;\:\: H_1 : $ No $H_0$, won't be rejected at a confidence level $95\%$.
  It also illsutrates the correlation between the two parameters, as the confidence region is elongated along a diagonal line. The negative slope provides 
  insight into the negative correlation between the two parameters.
  
  \item Use the figure of confidence intervals and confidence region to show the difference between testing statistical significance of regressors separately or jointly, and explain why this is so relevant under the presence of collinear regressors.
  
  \textbf{Answer:} 

The figure illustrates that joint testing can show us when collinear variables are jointly significant,
even if they are not individually significant.

\textbf{Individual tests:} Testing $H_0: \beta_3=0$ and 
$H_0: \beta_4=0$ separately uses the rectangular region formed by the 
individual 95\% confidence intervals (red dashed lines). From the regression output,
both intervals include zero: $x_3$ CI 
$= [-0.7740, 1.3733]$ and $x_4$ CI $= [-0.2945, 1.8582]$. Since the 
rectangle contains the origin $(0,0)$, we fail to reject both null 
hypotheses individually---neither $x_3$ nor $x_4$ appears statistically 
significant.

\textbf{Joint test:} Testing $H_0: \beta_3=\beta_4=0$ jointly uses the 
95\% confidence ellipse. The ellipse is much smaller than the rectangle 
due to the negative correlation between $\hat{\beta}_3$ and 
$\hat{\beta}_4$. If the origin $(0,0)$ falls outside the ellipse, we 
reject the joint null hypothesis, meaning $x_3$ and $x_4$ are jointly 
significant even though neither is individually significant.

\textbf{Collinearity:} When regressors are collinear, their 
coefficients are negatively correlated because if one increases, the other
must decrease to fit the same 
data. This creates the tilted ellipse. The individual tests ignore this 
correlation and use the wider rectangular region, making them overly 
conservative. The joint test correctly accounts for correlation, 
revealing that while we cannot precisely determine which variable drives 
the effect, we can confidently say that together they have a significant 
impact on $y$. This demonstrates why collinearity makes individual 
$t$-tests unreliable while joint $F$-tests remain valid.


\end{enumerate}

\subsection*{Part (b)}
Modify the script used to estimate now the same regression with data 
generated from the same dgp but now using a sample of 3500 observations.

\begin{enumerate}[label=(\roman*)]
  \item Surprised with how the estimates have changed? Rigorously justify.
  
  \textbf{Answer:} 

  \begin{table}[ht]
\centering
\caption{Regression Results with 95\% CI and n= 3500} 
\label{tab:regression2}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std..Error & Lower & Upper \\ 
  \hline
(Intercept) & 10.2294 & 0.1811 & 9.8744 & 10.5845 \\ 
  x2 & 0.4902  & 0.0078  & 0.4750 & 0.5054 \\ 
  x3 &0.5512 &  0.0685 & 0.4169 & 0.6855 \\ 
  x4 & 0.4475 & 0.0680 & 0.3143 & 0.5808 \\ 
   \hline
\end{tabular}
\end{table}
  
  Not surprised. The estimates have converged closer to the true parameter 
  values 
\textcolor{blue}{($\beta_3=\beta_4=0.5$, shown by purple dotted lines in Figure 
  \ref{fig:comparison}).} For $n=35$, the estimates were $\hat{\beta}_3=0.2996$ 
  and $\hat{\beta}_4=0.7818$, showing considerable sampling variability. 
  With $n=3500$, the estimates are much closer to 0.5 for both parameters, so now the estimates are more precise.
  
  By the Law of Large Numbers, as sample size increases, the OLS estimators 
  converge in probability to their true values. This is consistency: 
  $\text{plim}(\hat{\beta}_j) = \beta_j$ as $n \to \infty$. The larger 
  sample provides more information about the true relationship, reducing 
  the influence of random sampling variation. The collinearity between $x_3$ 
  and $x_4$ still exists (since $x_4 = x_3 + \text{noise}$ in both samples), 
  but with more observations, the estimator can better distinguish their 
  individual effects on $y$.
  
  
  \item Surprised of the change of the 95\% confidence intervals? Rigorously justify.
  
  \textbf{Answer:}

  Not surprised. As in (i) having more information about the population let you make better prediction because there's less variance, so less $se(\hat{\beta_k})$. That's why the intervals in this case are smaller.
  
  
  \item Surprised of the change of the 95\% confidence region for parameters $\beta_3$, $\beta_4$? Rigorously justify.
  
  \textbf{Answer:} 

  \begin{figure}[H]  % Capital H forces it exactly here
      \centering
      \includegraphics[width=0.4\textwidth]{Files/Ellipse2.png}
      \caption{95\% Confidence Region for $\beta_3$ and $\beta_4$ with n=3500}
      \label{fig:ellipse2}
    \end{figure}

  We see that the ellipse it has become shorter. That's not surprising, because this ellipse is based in the variance-covariance matrix of $x_3$ and $x_4$, and as the variance and covariance decrease when the size of the samples grow, the entire ellipse becomes smaller. 
  
  \item Using the variance decomposition expression for $\text{var}(\hat{\beta}_3 \mid X)$, or $\text{var}(\hat{\beta}_4 \mid X)$, discuss why increasing $n$ can explain the changes observed. Be specific.
  
  \textbf{Answer:} 

The variance decomposition is:
\begin{center}
    
$var(\hat{\beta_k}\mid X) = \sigma^2(X'X)^{-1}_{kk}= \sigma^2 \frac{1}{SST_k} \frac{1}{(1-R_k^2)}$.
\end{center}

Where $SST_k = \sum^n_{i=0}(x_{ik}-\bar{x}_k)^2$. 

If n increases then $SST_k$ also increases because it is the sum of the square of the errors of all the observations. The fact that $SST_k$ increases means that $\frac{1}{SST_k}$ decreases and so the $var(\hat{\beta_k}\mid X)$. 

Because the $se(\hat{\beta_k}) = \sqrt{var(\hat{\beta_k}\mid X)}$, $se(\hat{\beta_k})$ will also decrease, so the intervalse, defined as:

\begin{center}
   $[\hat{\beta_k} - t_{2.5\%(n-K)} se(\hat{\beta_k}), \hat{\beta_k} + t_{2.5\%(n-K)} se(\hat{\beta_k})]$ 
\end{center}

Will become shorter.

Also, as the ellipse is $z'Az $ with:

\begin{center}
    $A = [\sigma^2R(X'X)^{-1}R^{-1}]^{-1} =
\begin{pmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}
\end{pmatrix}
$
\end{center}

Where $\sigma^2R(X'X)^{-1}R^{-1}]^{-1}$ is the variance-covariance matrix of $\beta_3$ and $\beta_4$. We know that as bigger $a_{11}$ the horizontal axis is shorter, and as bigger $a_{22}$ the vertical axis is shorter. Because $A$ is the inverse matrix of the variance-covariance matrix of $\beta_3$ and $\beta_4$, if $var(\hat{\beta_3}\mid X)$ decrease $a_{11}$ increase so x-axis become shorter. By the same reasoning if $var(\hat{\beta_4}\mid X)$ decrease $a_{22}$ increase so y-axis of the ellipse become shorter.

  
\end{enumerate}

\subsection*{Part (c)}
Now, go back to the original script generating 35 observations, and modify the script so that now $x_{i3} + 2x_{i4} = 0$.

\begin{enumerate}[label=(\roman*)]
  \item Run the script again. Include the output in your answer.
  
  \textbf{Answer:} 

  \begin{table}[ht]
\centering
\caption{Regression Results with 95\% CI } 
\label{tab:regression2}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std..Error & Lower & Upper \\ 
  \hline
(Intercept) & 10.1838  &  2.2007 & 5.7012 & 14.6665 \\ 
  x2 & 0.4697  &  0.0999  & 0.2661 & 0.6733 \\ 
  x3 & 0.1985  &  0.1015 & -0.0083 & 0.4052 \\ 
  x4 & NA & NA & NA & NA \\ 
   \hline
\end{tabular}
\end{table}
  
  
  \item How many estimates did you get an estimate for $\beta_3$? And for $\beta_4$? You should be able to show, using the proper derivation, that in fact you got an infinite number of estimates for $\beta_3$ and $\beta_4$.
  
  \textbf{Answer:} 

  What happens here is that there are 2 regressors with perfect collinearity, and so $det(X'X)=0$ and $(X'X)^{-1}$ doesn't exist. That makes that we can't compute $\hat{\beta_k}$, and thats why $R$ put a NA in $x_4$ so we can compute the others.

  $R$ gives us one estimate for $\hat{\beta_3}$ and none for $\hat{\beta_4}$, but really, as $x_{i3} = -2x_{i4}$ we have the regression:

  \begin{center}
      $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} - \beta_4 0.5x_{i3} + \varepsilon_i$

      $= \beta_1 + \beta_2 x_{i2} + (\beta_3 -0.5\beta_4) x_{i3}+ \varepsilon_i$
  \end{center}

  So we have a regression with three independent regressors, and so we can find a unique estimate $\hat{\beta'}$ of $\beta' =\beta_3 -0.5\beta_4 $, thus $\hat{\beta'} =\hat{\beta_3} -0.5\hat{\beta_4} $ so there are infinite values of $\hat{\beta_3}$, $\hat{\beta_4}$ such that the equallity holds.
\end{enumerate}

\newpage

\section*{Question 2}
Data file \texttt{microsoft.csv} includes monthly data from May 1986 to April 2013 on $RP_{msft}$ (excess return of Microsoft stock), $RP_{s\&p}$ (excess return on the S\&P500 portfolio), $Dprod$ (variation of Industrial production), $Dinflation$ (change in inflation rate), $Dterm$ (change in interest rate) and $m1$ (an indicator variable that takes value 1 if $t$ is the month of January and 0 otherwise). The following regression is set to measure the reaction of the excess return of Microsoft stocks to changes in macroeconomic variables:
\[
RP_{msft,t} = \beta_1 + \beta_2 RP_{s\&p,t} + \beta_3 Dprod_t + \beta_4 Dinflation_t + \beta_5 Dterm_t + \beta_6 m1_t + \epsilon_t
\]

\begin{enumerate}[label=(\alph*)]
  \item Estimate the model above by OLS. Present the complete output (estimates, standard errors, p-values) as your answer.
  
  \textbf{Answer:} 

  \input{q2a_table.tex}
  
  \item The January effect states that on average, every else equal, the returns (or excess returns) are larger in the month of January than the rest of the months. Test, at $\alpha = 1\%$, the presence of the January effect using the exact $t$-test statistic. Would you say the data supports the presence of this effect?
  
  \textbf{Answer:} 

  We test for the presence of the January effect by examining whether the coefficient on the January dummy variable ($m_1$) is statistically significant.

  \textbf{Hypotheses:}
  \begin{align*}
      H_0: \beta_6 &= 0 \quad \text{(No January effect)} \\
      H_1: \beta_6 &\neq 0 \quad \text{(January effect exists)}
  \end{align*}

  We use a two-tailed $t$-test at significance level $\alpha = 0.01$.

    \textbf{Test Statistic:}
      \[
          t = \frac{\hat{\beta}_6}{\text{SE}(\hat{\beta}_6)} = \frac{5.44}{2.87} = 1.89
      \]

  With 318 degrees of freedom, the critical values are $\pm 2.59$. So, reject $H_0$ if $|t| > 2.59$ or equivalently if $p < 0.01$.
  The $t$-statistic is 1.89 with a $p$-value of 0.059. Since $|1.89| < 2.59$ and $p = 0.059 > 0.01$, we fail to reject the null hypothesis.
  Therefore, we do not have sufficient evidence at the 1\% significance level to conclude that there is a January effect in Microsoft stock returns.
  
  \item Aside from normality, list the assumptions needed to justify the use of the $t$ test statistic. Justify your answer.
  
  \textbf{Answer:} 
  
  \item For the test you performed in questions (2b), you were asked to use the $t$ test statistic. Using this test requires, among others, for disturbances to be normally distributed. One of the available tests of normality of the distribution of a given random variable is the Jarque-Bera test. Under the null hypothesis of normality, the Jarque-Bera (JB) test statistic is:
  \[
  JB \equiv \frac{n}{6}\left[sk^2 + \frac{(kur - 3)^2}{4}\right] \stackrel{a}{\sim} \chi^2(2),
  \]
  where $sk$ is the sample coefficient of skewness of the variable and $kur$ is its sample coefficient of kurtosis. Using a significance level of 1\%, draw (by hand absolutely fine) the distribution of JB under $H_0$ and the corresponding acceptance and rejection regions. Provide an intuition for the location of the acceptance region.
  
  \textbf{Answer:} 
  
  \item Now, we want to test for the presence of normality in our disturbances using the JB test. Ideally, to test normality of disturbances, JB test should be applied to a sample of disturbances, but, given that they are unobservable, the JB test is usually applied to our OLS residuals. Explain how, if all the assumptions regarding the dgp for consistency of OLS estimator are met, it would be justified for $\hat{\epsilon}_t$'s to take the place of $\epsilon_t$'s to perform the test.
  
  \textbf{Answer:} 
  
  \item Perform the JB test on the OLS residuals. What do you conclude? Comment.
  
  \textbf{Answer:} 
  
  \item Repeat the test performed in (2b) using the asymptotic $T$-test statistic. Use a 1\% significance level.
  
  \textbf{Answer:} 
  
  \item Is the use of asymptotic tests justified in this case? Rigorously argue.
  
  \textbf{Answer:} 
  
  \item Consider the following statement: ``Using the exact $t$ test statistic leads to slightly more conservative inference, because we get larger acceptance regions and larger $p$-values than if we used the asymptotic version.'' Do you agree? Rigorously argue.
  
  \textbf{Answer:} 
\end{enumerate}

\end{document}