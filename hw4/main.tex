\documentclass[12pt,a4paper]{article}

\input{LaTex/packages.tex}

\usepackage{fancyhdr}
\setlength{\headheight}{15pt}  % Increase header height
\addtolength{\topmargin}{-3pt}  % Adjust top margin to compensate

\begin{document}

% Title page
\begin{titlepage}
\centering
\includegraphics[width=0.75\textwidth]{LaTex/imgs/bse_logo.pdf}
\par\vspace{0.75cm}
	{\huge\bfseries Assignment 4 \par}
    {\large\bfseries Foundations of Econometrics \\
                        Group 7\par}
	\vspace{0.25cm}
    \noindent\rule{\textwidth}{1pt}
    {\Large 
        \par}
    \noindent\rule{\textwidth}{1pt}
	\vfill
	{\large \today\par}
\end{titlepage}
\newpage

\section*{Question 1}
Class slide 3(33) (Unit 3) illustrated, via simulation, the effects of collinearity. The script used to generate the sample is included in file \texttt{data33.R}.

\subsection*{Part (a)}
\begin{enumerate}[label=(\roman*)]
  \item Estimate the regression model included in the slide, presenting OLS estimates and the 95\% confidence intervals for each parameter; Include the output in your answer.
  
  \textbf{Answer:} 
 
\begin{table}[ht]
    \centering
    \caption{Regression Results with 95\% CI} 
    \label{tab:regression}
  \begin{tabular}{rrrrr}
    \hline
    & Estimate & Std..Error & Lower & Upper \\ 
    \hline
    (Intercept) & 7.9365 & 1.6426 & 4.5863 & 11.2867 \\ 
    x2 & 0.5953 & 0.0747 & 0.4429 & 0.7476 \\ 
    x3 & 0.2996 & 0.5264 & -0.7740 & 1.3733 \\ 
    x4 & 0.7818 & 0.5277 & -0.2945 & 1.8582 \\ 
    \hline
  \end{tabular}
\end{table}

  
  \item Using \texttt{confidenceEllipse()} function, or equivalent, draw the 95\% confidence region for parameters $\beta_3$, $\beta_4$. Include also in the drawing the confidence intervals for each parameter.
  
  \textbf{Answer:} 

    \begin{figure}[H]  % Capital H forces it exactly here
      \centering
      \includegraphics[width=0.5\textwidth]{Files/q1ii_plot.png}
      \caption{95\% Confidence Region for $\beta_3$ and $\beta_4$}
      \label{fig:ellipse}
    \end{figure}

  \item Describe what the confidence region you just drew provides.
  
  \textbf{Answer:} 

  The ellipse we drew illustrates the joint confiendence region, while the red lines show the individual confidence intervals.
  It represents every combination of the two parameters that falls in the 95\% confidence region. It also illsutrates the
  correlation between the two parameters, as the confidence region is elongated along a diagonal line. The negative slope provides 
  insight into the negative correlation between the two parameters.
  
  \item Use the figure of confidence intervals and confidence region to show the difference between testing statistical significance of regressors separately or jointly, and explain why this is so relevant under the presence of collinear regressors.
  
  \textbf{Answer:} 

The figure illustrates that joint testing can show us when collinear variables are jointly significant,
even if they are not individually significant.

\textbf{Individual tests:} Testing $H_0: \beta_3=0$ and 
$H_0: \beta_4=0$ separately uses the rectangular region formed by the 
individual 95\% confidence intervals (red dashed lines). From the regression output,
both intervals include zero: $x_3$ CI 
$= [-0.7740, 1.3733]$ and $x_4$ CI $= [-0.2945, 1.8582]$. Since the 
rectangle contains the origin $(0,0)$, we fail to reject both null 
hypotheses individually---neither $x_3$ nor $x_4$ appears statistically 
significant.

\textbf{Joint test:} Testing $H_0: \beta_3=\beta_4=0$ jointly uses the 
95\% confidence ellipse. The ellipse is much smaller than the rectangle 
due to the negative correlation between $\hat{\beta}_3$ and 
$\hat{\beta}_4$. If the origin $(0,0)$ falls outside the ellipse, we 
reject the joint null hypothesis, meaning $x_3$ and $x_4$ are jointly 
significant even though neither is individually significant.

\textbf{Collinearity:} When regressors are collinear, their 
coefficients are negatively correlated because if one increases, the other
must decrease to fit the same 
data. This creates the tilted ellipse. The individual tests ignore this 
correlation and use the wider rectangular region, making them overly 
conservative. The joint test correctly accounts for correlation, 
revealing that while we cannot precisely determine which variable drives 
the effect, we can confidently say that together they have a significant 
impact on $y$. This demonstrates why collinearity makes individual 
$t$-tests unreliable while joint $F$-tests remain valid.


\end{enumerate}

latex\subsection*{Part (b)}
Modify the script used to estimate now the same regression with data 
generated from the same dgp but now using a sample of 3500 observations.

\begin{enumerate}[label=(\roman*)]
  \item Surprised with how the estimates have changed? Rigorously justify.
  
  \textbf{Answer:} 
  
  Not surprised. The estimates have converged closer to the true parameter 
  values ($\beta_3=\beta_4=0.5$, shown by purple dotted lines in Figure 
  \ref{fig:comparison}). For $n=35$, the estimates were $\hat{\beta}_3=0.2996$ 
  and $\hat{\beta}_4=0.7818$, showing considerable sampling variability. 
  With $n=3500$, the estimates are much closer to 0.5 for both parameters.
  
  By the Law of Large Numbers, as sample size increases, the OLS estimators 
  converge in probability to their true values. This is consistency: 
  $\text{plim}(\hat{\beta}_j) = \beta_j$ as $n \to \infty$. The larger 
  sample provides more information about the true relationship, reducing 
  the influence of random sampling variation. The collinearity between $x_3$ 
  and $x_4$ still exists (since $x_4 = x_3 + \text{noise}$ in both samples), 
  but with more observations, the estimator can better distinguish their 
  individual effects on $y$.
  
  
  \item Surprised of the change of the 95\% confidence intervals? Rigorously justify.
  
  \textbf{Answer:} 

  Not surprised. The confidence intervals have become much narrower with the larger sample. 
  For $n=35$: $\beta_3$ CI = [-0.7740, 1.3733] with width = 2.15, and $\beta_4$ CI = [-0.2945, 1.8582] with width = 2.15.
  With $n=3500$, the confidence intervals should be approximately $\sqrt{3500/35} = 10$ times narrower, giving widths around 0.215.
  This follows from the asymptotic distribution of OLS estimators.

  \item Surprised of the change of the 95\% confidence region for parameters $\beta_3$, $\beta_4$? Rigorously justify.
  
  \textbf{Answer:} 
  
  Not surprised. The confidence ellipse has shrunk dramatically while maintaining its elongated shape oriented along the same diagonal.
  The area of the confidence ellipse is proportional to $|\text{Var}(\hat{\beta}_3, \hat{\beta}_4)|$, which is proportional to $1/n$. Therefore, increasing sample size from 35 to 3500 (a factor of 100) reduces the ellipse area by a factor of 100.
  The ellipse remains tilted because the negative correlation between $\hat{\beta}_3$ and $\hat{\beta}_4$ persists—it's determined by the correlation between $x_3$ and $x_4$ in the DGP, which doesn't change with sample size. However, the smaller ellipse means:

  \begin{itemize}
  \item Higher precision in joint estimation of $\beta_3$ and $\beta_4$
  \item The point $(0.5, 0.5)$ (true values) is much more likely to be contained in the ellipse
  \item If the origin $(0,0)$ was near the boundary for $n=35$, it may now fall outside the ellipse for $n=3500$, making the joint test reject $H_0: \beta_3 = \beta_4 = 0$ even though individual tests might still fail to reject
  \end{itemize}

  \item Using the variance decomposition expression for $\text{var}(\hat{\beta}_3 \mid X)$, or $\text{var}(\hat{\beta}_4 \mid X)$, discuss why increasing $n$ can explain the changes observed. Be specific.
  
  \textbf{Answer:} 

  The variance of $\hat{\beta}_3$ conditional on $X$ is:
  $$\text{Var}(\hat{\beta}_3 | X) = \sigma^2 [(X'X)^{-1}]_{33} = \frac{\sigma^2}{\sum_{i=1}^{n}(x_{i3} - \bar{x}_3)^2 (1 - R_3^2)}$$
  where $R_3^2$ is the R-squared from regressing $x_3$ on all other regressors (including $x_4$).

  This can be decomposed as:
  $$\text{Var}(\hat{\beta}_3 | X) = \sigma^2 \cdot \frac{1}{\sum_{i=1}^{n}(x_{i3} - \bar{x}_3)^2} \cdot \frac{1}{1 - R_3^2} = \sigma^2 \cdot \frac{1}{n \cdot \text{Var}(x_3)} \cdot \text{VIF}_3$$

  Three components:
  \begin{enumerate}
    \item $\sigma^2$: Error variance (constant across samples if same DGP)
    \item $\frac{1}{n \cdot \text{Var}(x_3)}$: Decreases linearly with $n$
    \item $\text{VIF}_3 = \frac{1}{1-R_3^2}$: Variance Inflation Factor due to collinearity (constant for same DGP)
  \end{enumerate}

\textbf{Effect of increasing $n$ from 35 to 3500:}
  \begin{itemize}
    \item The term $\frac{1}{n}$ decreases by factor of 100
    \item Therefore $\text{Var}(\hat{\beta}_3 | X)$ decreases by factor of 100
    \item Standard errors decrease by factor of $\sqrt{100} = 10$
    \item Confidence intervals become 10 times narrower
    \item The confidence ellipse shrinks by factor of 100 in area
\end{itemize}

\end{enumerate}

\subsection*{Part (c)}
Now, go back to the original script generating 35 observations, and modify the script so that now $x_{i3} + 2x_{i4} = 0$.

\begin{enumerate}[label=(\roman*)]
  \item Run the script again. Include the output in your answer.
  
  \textbf{Answer:} 

 \input{regression_table_1ci.tex}
  
  \item How many estimates did you get an estimate for $\beta_3$? And for $\beta_4$? You should be able to show, using the proper derivation, that in fact you got an infinite number of estimates for $\beta_3$ and $\beta_4$.
  
  \textbf{Answer:} 

  From the regression output, we obtained \textbf{one estimate} for $\beta_3$ (the one R reported) and \textbf{zero estimates} for $\beta_4$ (R returned NA). 
  However, mathematically, there are \textbf{infinitely many} valid estimates for both $\beta_3$ and $\beta_4$.

  \textbf{Derivation:}

      The OLS estimator solves the normal equations:
      $$(X'X)\hat{\beta} = X'y$$

    When $x_3 + 2x_4 = 0$ (perfect collinearity), the matrix $X'X$ is singular (non-invertible) because 
    its columns are linearly dependent. This means the normal equations have either no solution or infinitely many solutions. 
    Since the least squares problem always has a solution (we can always minimize SSR), it must have infinitely many solutions.

  \textbf{To show infinite solutions:}

  Suppose $(\hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3, \hat{\beta}_4)$ is a solution. 
  The fitted values are:
  $$\hat{y}_i = \hat{\beta}_1 + \hat{\beta}_2 x_{i2} + \hat{\beta}_3 x_{i3} + \hat{\beta}_4 x_{i4}$$

  Now consider an alternative set of coefficients:
  $$\beta_3^* = \hat{\beta}_3 + t, \quad \beta_4^* = \hat{\beta}_4 + 2t$$

  for any constant $t \in \mathbb{R}$. The fitted values become:
  \begin{align*}
    \hat{y}_i^* &= \hat{\beta}_1 + \hat{\beta}_2 x_{i2} + (\hat{\beta}_3 + t) x_{i3} + (\hat{\beta}_4 + 2t) x_{i4} \\
    &= \hat{\beta}_1 + \hat{\beta}_2 x_{i2} + \hat{\beta}_3 x_{i3} + \hat{\beta}_4 x_{i4} + t x_{i3} + 2t x_{i4} \\
    &= \hat{\beta}_1 + \hat{\beta}_2 x_{i2} + \hat{\beta}_3 x_{i3} + \hat{\beta}_4 x_{i4} + t(x_{i3} + 2x_{i4}) \\
    &= \hat{\beta}_1 + \hat{\beta}_2 x_{i2} + \hat{\beta}_3 x_{i3} + \hat{\beta}_4 x_{i4} + t \cdot 0 \\
    &= \hat{y}_i
  \end{align*}

Since $x_{i3} + 2x_{i4} = 0$ for all $i$, the fitted values (and thus the SSR) are identical for any value of $t$. 
Therefore, $(\hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3 + t, \hat{\beta}_4 + 2t)$ is also a valid OLS solution for \textbf{any} $t \in \mathbb{R}$.

\textbf{Conclusion:} There are infinitely many combinations of $(\beta_3, \beta_4)$ that minimize the sum of squared residuals. 
We cannot uniquely identify $\beta_3$ and $\beta_4$ separately—only their weighted combination $\beta_3 x_3 + \beta_4 x_4$ is identified. 
This is why R drops one variable and returns NA: it's R's way of acknowledging that the parameters cannot be uniquely estimated.

\newpage

\section*{Question 2}
Data file \texttt{microsoft.csv} includes monthly data from May 1986 to April 2013 on $RP_{msft}$ (excess return of Microsoft stock), $RP_{s\&p}$ (excess return on the S\&P500 portfolio), $Dprod$ (variation of Industrial production), $Dinflation$ (change in inflation rate), $Dterm$ (change in interest rate) and $m1$ (an indicator variable that takes value 1 if $t$ is the month of January and 0 otherwise). The following regression is set to measure the reaction of the excess return of Microsoft stocks to changes in macroeconomic variables:
\[
RP_{msft,t} = \beta_1 + \beta_2 RP_{s\&p,t} + \beta_3 Dprod_t + \beta_4 Dinflation_t + \beta_5 Dterm_t + \beta_6 m1_t + \epsilon_t
\]

\begin{enumerate}[label=(\alph*)]
  \item Estimate the model above by OLS. Present the complete output (estimates, standard errors, p-values) as your answer.
  
  \textbf{Answer:} 

  \input{q2a_table.tex}
  
  \item The January effect states that on average, every else equal, the returns (or excess returns) are larger in the month of January than the rest of the months. Test, at $\alpha = 1\%$, the presence of the January effect using the exact $t$-test statistic. Would you say the data supports the presence of this effect?
  
  \textbf{Answer:} 

  We test for the presence of the January effect by examining whether the coefficient on the January dummy variable ($m_1$) is statistically significant.

  \textbf{Hypotheses:}
  \begin{align*}
      H_0: \beta_6 &= 0 \quad \text{(No January effect)} \\
      H_1: \beta_6 &\neq 0 \quad \text{(January effect exists)}
  \end{align*}

  We use a two-tailed $t$-test at significance level $\alpha = 0.01$.

    \textbf{Test Statistic:}
      \[
          t = \frac{\hat{\beta}_6}{\text{SE}(\hat{\beta}_6)} = \frac{5.44}{2.87} = 1.89
      \]

  With 318 degrees of freedom, the critical values are $\pm 2.59$. So, reject $H_0$ if $|t| > 2.59$ or equivalently if $p < 0.01$.
  The $t$-statistic is 1.89 with a $p$-value of 0.059. Since $|1.89| < 2.59$ and $p = 0.059 > 0.01$, we fail to reject the null hypothesis.
  Therefore, we do not have sufficient evidence at the 1\% significance level to conclude that there is a January effect in Microsoft stock returns.

  \item Aside from normality, list the assumptions needed to justify the use of the $t$ test statistic. Justify your answer.
  
  \textbf{Answer:} 
  
  \item For the test you performed in questions (2b), you were asked to use the $t$ test statistic. Using this test requires, among others, for disturbances to be normally distributed. One of the available tests of normality of the distribution of a given random variable is the Jarque-Bera test. Under the null hypothesis of normality, the Jarque-Bera (JB) test statistic is:
  \[
  JB \equiv \frac{n}{6}\left[sk^2 + \frac{(kur - 3)^2}{4}\right] \stackrel{a}{\sim} \chi^2(2),
  \]
  where $sk$ is the sample coefficient of skewness of the variable and $kur$ is its sample coefficient of kurtosis. Using a significance level of 1\%, draw (by hand absolutely fine) the distribution of JB under $H_0$ and the corresponding acceptance and rejection regions. Provide an intuition for the location of the acceptance region.
  
  \textbf{Answer:} 

  \textbf{Hypotheses:}
  \begin{align*}
    H_0&: \text{Residuals are normally distributed} \\
    H_1&: \text{Residuals are not normally distributed}
  \end{align*}

  \textbf{Test Statistic:}
    Under $H_0$, the Jarque-Bera statistic follows a $\chi^2(2)$ distribution:
    \[
      JB = \frac{n}{6}\left[sk^2 + \frac{(kur-3)^2}{4}\right] \sim \chi^2(2)
    \]
    where $sk$ is the sample skewness and $kur$ is the sample kurtosis.

  \textbf{Results:}
    \begin{itemize}
    \item Sample size: $n = 324$
    \item Skewness: $sk = -2.54$
    \item Kurtosis: $kur = 13.40$
    \item $JB = 1809.21$
    \item Critical value: $\chi^2_{0.01}(2) = 9.21$
    \item $p$-value: $< 0.0001$
  \end{itemize}
  
  \textbf{Decision:} Since $JB = 1809.21 > 9.21$, we strongly reject the null hypothesis at the 1\% significance level.


  \item Now, we want to test for the presence of normality in our disturbances using the JB test. Ideally, to test normality of disturbances, JB test should be applied to a sample of disturbances, but, given that they are unobservable, the JB test is usually applied to our OLS residuals. Explain how, if all the assumptions regarding the dgp for consistency of OLS estimator are met, it would be justified for $\hat{\epsilon}_t$'s to take the place of $\epsilon_t$'s to perform the test.
  
  \textbf{Answer:} 
  
  \item Perform the JB test on the OLS residuals. What do you conclude? Comment.
  
  \textbf{Answer:} 
  
  \item Repeat the test performed in (2b) using the asymptotic $T$-test statistic. Use a 1\% significance level.
  
  \textbf{Answer:} 
  
  \item Is the use of asymptotic tests justified in this case? Rigorously argue.
  
  \textbf{Answer:} 
  
  \item Consider the following statement: ``Using the exact $t$ test statistic leads to slightly more conservative inference, because we get larger acceptance regions and larger $p$-values than if we used the asymptotic version.'' Do you agree? Rigorously argue.
  
  \textbf{Answer:} 
\end{enumerate}

\end{document}